= llama-docker

experimenting with locallly run containerized LLM

.Target:
* [x] Steamdeck (podman)
* [ ] pinebook pro (docker)
* [ ] raspi (docker)
* [x] Macbook pro (docker/colima)
