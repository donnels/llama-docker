= llama-docker
:toc: right

experimenting with locallly run containerized LLM

== Targets
.Target:
* [x] Steamdeck (podman)
** Works well
* [ ] pinebook pro (docker)
** Not really a good fit
* [ ] raspi (docker)
** Pending test
* [x] Macbook pro (docker/colima)

== How to
This section is a quick run down of how to:

* pull the repo
* build the image
* run the container
* connect to the container with a browser
* connect into the container to poke around

=== Requirements
In general it's easy enough to build and run a docker container and all it needs are:

* Docker
** OR an equivalent like podman or colima (MAC only via brew)
* git 
** OR you can pull a ZIP of the repo from the website directly

=== Pulling this repository
The following is a command line way of pulling the repository with the build instructions:

.Clone the repository
[source,bash]
----
# change to a directory where the repository is to have its abode.
# then clone the repo
git clone https://github.com/donnels/llama-docker.git
cd llama-docker
----

=== Building the Image
After having pulled the repository one can `build` the image (which is `run` later as the container).

.Docker Build
[source,bash]
----
cd docker
docker build -t llama .
----

.Output
----
[+] Building 2.9s (10/10) FINISHED                                                                                                
 => [internal] load build definition from Dockerfile                                                                         0.2s
 => => transferring dockerfile: 1.06kB                                                                                       0.0s
 => [internal] load .dockerignore                                                                                            0.2s
 => => transferring context: 64B                                                                                             0.0s
 => [internal] load metadata for docker.io/library/debian:stable-slim                                                        2.3s
 => [1/5] FROM docker.io/library/debian:stable-slim@sha256:382967fd7c35a0899ca3146b0b73d0791478fba2f71020c7aa8c27e3a4f26672  0.0s
 => [internal] load build context                                                                                            0.0s
 => => transferring context: 35B                                                                                             0.0s
 => CACHED [2/5] RUN apt-get update     && apt-get upgrade -y     && apt-get install -y         net-tools iproute2 procps    0.0s
 => CACHED [3/5] WORKDIR /data                                                                                               0.0s
 => CACHED [4/5] RUN wget $wgetOpts https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-q4.llamafi  0.0s
 => CACHED [5/5] COPY . .                                                                                                    0.0s
 => exporting to image                                                                                                       0.1s
 => => exporting layers                                                                                                      0.0s
 => => writing image sha256:fe12408ee5faa7934301f7288415c21ffb8cc0b5767b23e330a050867e13d1bc                                 0.0s
 => => naming to docker.io/library/test1:latest                                                                              0.0s
----

The above was from a cached build and is in contrast to a fresh build.
A fresh build, the first time, will take a while longer depending on the speed of the internet connexion as the model is a few Gigabytes. A low speed Internet connexion may take approximately an hour (12Mbit).

.Checking on the size of the image
[source,bash]
----
docker image ls         
----

.Output
----

----

.Checking how the image is built up
[source,bash]
----
docker history llama  
----

.Output
----
  
----

=== Running the Container
A container is just an instantiation of an image.

.Docker Run
[source,bash]
----
docker run --rm --name llama -p8080:8080 -it llama
----

.The above command:
* runs the container with `run`
* `--rm` removes the container after it's stopped.
* gives the container, from the image `llama`, a name `--name llama`.
* exposes the containers ports on the local system with `-p8080:8080`.
* makes the container connect to the CLI it was started on `-it` so that a `CTRL-c` can later stop it and you can see what it's up to while it's running.

.output
----
note: if you have an AMD or NVIDIA GPU then you need to pass -ngl 9999 to enable GPU offloading
{"build":1500,"commit":"a30b324","function":"server_cli","level":"INFO","line":2869,"msg":"build info","tid":"10437056","timestamp":1724063878}
{"function":"server_cli","level":"INFO","line":2872,"msg":"system info","n_threads":2,"n_threads_batch":-1,"system_info":"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | ","tid":"10437056","timestamp":1724063878,"total_threads":2}
{"function":"load_model","level":"INFO","line":435,"msg":"Multi Modal Mode Enabled","tid":"10437056","timestamp":1724063878}
clip_model_load: model name:   openai/clip-vit-large-patch14-336
clip_model_load: description:  image encoder for LLaVA
clip_model_load: GGUF version: 3
clip_model_load: alignment:    32
clip_model_load: n_tensors:    377
clip_model_load: n_kv:         19
clip_model_load: ftype:        q4_0
clip_model_load: loaded meta data with 19 key-value pairs and 377 tensors from llava-v1.5-7b-mmproj-Q4_0.gguf
clip_model_load: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
clip_model_load: - kv   0:                       general.architecture str              = clip
clip_model_load: - kv   1:                      clip.has_text_encoder bool             = false
clip_model_load: - kv   2:                    clip.has_vision_encoder bool             = true
clip_model_load: - kv   3:                   clip.has_llava_projector bool             = true
clip_model_load: - kv   4:                          general.file_type u32              = 2
clip_model_load: - kv   5:                               general.name str              = openai/clip-vit-large-patch14-336
clip_model_load: - kv   6:                        general.description str              = image encoder for LLaVA
clip_model_load: - kv   7:                     clip.vision.image_size u32              = 336
clip_model_load: - kv   8:                     clip.vision.patch_size u32              = 14
clip_model_load: - kv   9:               clip.vision.embedding_length u32              = 1024
clip_model_load: - kv  10:            clip.vision.feed_forward_length u32              = 4096
clip_model_load: - kv  11:                 clip.vision.projection_dim u32              = 768
clip_model_load: - kv  12:           clip.vision.attention.head_count u32              = 16
clip_model_load: - kv  13:   clip.vision.attention.layer_norm_epsilon f32              = 0.000010
clip_model_load: - kv  14:                    clip.vision.block_count u32              = 23
clip_model_load: - kv  15:                     clip.vision.image_mean arr[f32,3]       = [0.481455, 0.457828, 0.408211]
clip_model_load: - kv  16:                      clip.vision.image_std arr[f32,3]       = [0.268630, 0.261303, 0.275777]
clip_model_load: - kv  17:                              clip.use_gelu bool             = false
clip_model_load: - kv  18:               general.quantization_version u32              = 2
clip_model_load: - type  f32:  235 tensors
clip_model_load: - type  f16:    1 tensors
clip_model_load: - type q4_0:  141 tensors
clip_model_load: CLIP using CPU backend
clip_model_load: text_encoder:   0
clip_model_load: vision_encoder: 1
clip_model_load: llava_projector:  1
clip_model_load: model size:     169.18 MB
clip_model_load: metadata size:  0.17 MB
clip_model_load: params backend buffer size =  169.18 MB (377 tensors)
clip_model_load: compute allocated memory: 32.89 MB
llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llava-v1.5-7b-Q4_K.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.17 MiB
llm_load_tensors:        CPU buffer size =  3891.24 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 2048
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.14 MiB
llama_new_context_with_model:        CPU compute buffer size =   164.01 MiB
llama_new_context_with_model: graph nodes  = 1030
llama_new_context_with_model: graph splits = 1
{"function":"initialize","level":"INFO","line":489,"msg":"initializing slots","n_slots":1,"tid":"10437056","timestamp":1724063912}
{"function":"initialize","level":"INFO","line":498,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"10437056","timestamp":1724063912}
{"function":"server_cli","level":"INFO","line":3090,"msg":"model loaded","tid":"10437056","timestamp":1724063912}

llama server listening at http://172.17.0.2:8080
llama server listening at http://127.0.0.1:8080

In the sandboxing block!
{"function":"server_cli","hostname":"0.0.0.0","level":"INFO","line":3213,"msg":"HTTP server listening","port":"8080","tid":"10437056","timestamp":1724063912}
{"function":"update_slots","level":"INFO","line":1659,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"10437056","timestamp":1724063912}
----

=== Connecting to the container with a browser
At this point we should be able to connect to the container, with a browser, as follows from the system running docker:

* link:http://localhost:8080[^]

.Screenshot of initial web interface
image::./images/screenshot-llama-localhost-start-screen.png[]

=== Connecting INTO the container to check logs etc.
Connecting to the web interface allows using the container as designed with a browser.
In contrast to that connecting into to the container allows looking at logs and checking the contents of the container and potentially adding further components to debug or analyze the contents.

.Access into the container
[source,bash]
----
docker exec -it llama bash
----

This will give you CLI access into the container. Typical debian commands will work here and `aptitude` is pre-installed to help looking for further packages for those not wanting to use `apt-get update`and `apt-get install`.

.Checking the model size
[source,bash]
----
root@098c4d422da8:/data# du -sh *
4.0K	ENTRYPOINT.sh
4.0K	llama.log
4.0G	llava-v1.5-7b-q4.llamafile
----

.Checking network activity (idle)
[source,bash]
----
root@098c4d422da8:/data# netstat -na
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State      
tcp        0      0 0.0.0.0:8080            0.0.0.0:*               LISTEN     
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
----

The above shows the system just sitting there listening for a connexion.

.Checking network activity (active)
[source,bash]
----
root@098c4d422da8:/data# netstat -na
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State      
tcp        0      0 0.0.0.0:8080            0.0.0.0:*               LISTEN     
tcp        0      0 172.17.0.2:8080         172.17.0.1:60160        ESTABLISHED
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   Path
----

With an active query running the system shows clearly that it's making up the answers all on its own and isn't running off to the Internet to ask some service.

.Looking at processes
[source,bash]
----
root@098c4d422da8:/data# ps auxf
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root          31  0.0  0.1   4188  2304 pts/1    Ss   10:48   0:00 bash
root          57  0.0  0.2   8088  4096 pts/1    R+   12:29   0:00  \_ ps auxf
root           1  0.0  0.0   3924   384 pts/0    Ss+  10:39   0:00 bash /data/ENTRYPOINT.sh
root           7  2.5 72.7 9629008 1455532 pts/0 Sl+  10:39   2:47 /root/.ape-1.10 /data/llava-v1.5-7b-q4.llamafile --host 0.0.0.0
----

Clearly showing that there is just the model running as a single entity.

== Questions and Answers
This section just goes into some of the questions that may arise when you start using an LLM.

=== What does an LLM do?
It has been pre trained and has generated the "model". The model then guesses what comes next. It's more or less a statistical model which predicts the next word... over and over thereby either filling in the blanks or rambling on. It does that based on the context you give it so that the more information you give it the closer it get's to something you might consider useful. And to be clear: It knows nothing and cannot think per se.

=== Where does it get the answers from?
It generates them based on the input. The less it "knows" about a topic the more prone it is to making up something outrageous (hallucination). But to be honest all answers are made up and that's why they should not be trusted. It's just that some of the made up answers are spot on or close enough as they are probable and the less it knows the more wildly it stabs in the dark. And it does stab in the dark.
If you enable the probability display it will show you, for each token/word, how probable it "thinks" it is.

=== Why are the answers different each time I ask the same question?
The `temperature` setting introduces some randomness (or creativity). If the temperature is set high the LLM can drift into the realms of the unlikely. The higher the temperature the more creative the answers can get.
This can be good and also bad. Beware.
You can enable the LLM to display the probabilities so that you can view at which points in the answer it deviated from a zero temperature and how.

=== Where does the data that I enter go?
Into the chatbot in the container and nowhere else. When the container is stopped anything you have entered is lost (in the sense of has evaporated).

=== Can I test things here?
Yes. You can try out things like prompt engineering and you can try to get the LLM to answer questions it's been told not to. There may be plenty in the training data that is probably not that nice, so here also beware. And the environment does not hold a grudge so you can try out many things without being banned for violating the terms and conditions of a provider.


