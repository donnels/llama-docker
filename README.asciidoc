= llama-docker

experimenting with locallly run containerized LLM

.Target:
* [ ] Steamdeck (podman?)
* [ ] pinebook pro (docker)
* [ ] raspi (docker)
* [x] Macbook pro (docker/colima)
