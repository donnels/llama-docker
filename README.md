# llama-docker
experimenting with locallly run containerized LLM
